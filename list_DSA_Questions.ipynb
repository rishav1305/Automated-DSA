{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from autogen import ConversableAgent\n",
    "from autogen.coding import LocalCommandLineCodeExecutor\n",
    "\n",
    "work_dir = Path(\"coding\")\n",
    "work_dir.mkdir(exist_ok=True)\n",
    "\n",
    "executor = LocalCommandLineCodeExecutor(work_dir=work_dir)\n",
    "\n",
    "code_executor_agent = ConversableAgent(\n",
    "    name=\"code_executor_agent\",\n",
    "    llm_config=False,\n",
    "    code_execution_config={\n",
    "        \"executor\": executor,\n",
    "    },\n",
    "    human_input_mode=\"ALWAYS\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure LLM using OllamaConfig\n",
    "config_list = [{\n",
    "    \"model\": \"deepseek-r1:14b\",\n",
    "    \"base_url\": \"http://localhost:11434\",\n",
    "    \"api_type\": \"ollama\"\n",
    "}]\n",
    "\n",
    "# Configure the assistant\n",
    "llm_config = {\n",
    "    \"seed\": 42,\n",
    "    \"config_list\": config_list,\n",
    "    \"temperature\": 0.1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code writer agent's system message is to instruct the LLM on how to\n",
    "# use the Jupyter code executor with IPython kernel.\n",
    "code_writer_system_message = \"\"\"\n",
    "You have been given coding capability to solve tasks using Python code.\n",
    "In the following cases, suggest python code (in a python coding block) or shell script (in a sh coding block) for the user to execute.\n",
    "    1. When you need to collect info, use the code to output the info you need, for example, browse or search the web, download/read a file, print the content of a webpage or a file, get the current date/time, check the operating system. After sufficient info is printed and the task is ready to be solved based on your language skill, you can solve the task by yourself.\n",
    "    2. When you need to perform some task with code, use the code to perform the task and output the result. Finish the task smartly.\n",
    "Solve the task step by step if you need to. If a plan is not provided, explain your plan first. Be clear which step uses code, and which step uses your language skill.\n",
    "When using code, you must indicate the script type in the code block. The user cannot provide any other feedback or perform any other action beyond executing the code you suggest. The user can't modify your code. So do not suggest incomplete code which requires users to modify. Don't use a code block if it's not intended to be executed by the user.\n",
    "If you want the user to save the code in a file before executing it, put # filename: <filename> inside the code block as the first line. Don't include multiple code blocks in one response. Do not ask users to copy and paste the result. Instead, use 'print' function for the output when relevant. Check the execution result returned by the user.\n",
    "For any script, always include the pip install for all imports to reduce the chance of errors. Also include the correct in each prompt response to ensure the user can execute the code correctly.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "\n",
    "code_writer_agent = ConversableAgent(\n",
    "    \"code_writer\",\n",
    "    system_message=code_writer_system_message,\n",
    "    llm_config=llm_config,\n",
    "    code_execution_config=False,  # Turn off code execution for this agent.\n",
    "    max_consecutive_auto_reply=5,\n",
    "    human_input_mode=\"ALWAYS\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mcode_executor_agent\u001b[0m (to code_writer):\n",
      "\n",
      "\n",
      "    Scrape the contents from this URL: https://neetcode.io/practice?tab=neetcode150\n",
      "    Fetch all the <tables> tags available in the page. Convert them to pandas dataframes.\n",
      "    Concat all the dataframes into a single dataframe and write it to a CSV file.\n",
      "    \n",
      "    \n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> NO HUMAN INPUT RECEIVED.\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mcode_writer\u001b[0m (to code_executor_agent):\n",
      "\n",
      "<think>\n",
      "Alright, I need to figure out how to help the user scrape the contents from the given URL using Python. The task is to fetch all the <table> tags, convert them into pandas DataFrames, concatenate them, and then save the result into a CSV file.\n",
      "\n",
      "First, I'll consider the tools needed for web scraping. Beautiful Soup is a good choice for parsing HTML content. To fetch the webpage, requests library will be used. For handling data, pandas is essential since we're dealing with tables and DataFrames.\n",
      "\n",
      "I should outline the steps clearly:\n",
      "\n",
      "1. **Fetch the webpage content**: Use requests.get() to retrieve the HTML from the URL.\n",
      "2. **Parse the HTML**: Use Beautiful Soup to parse the HTML content and find all table elements.\n",
      "3. **Convert tables to DataFrames**: Loop through each table, extract rows and columns, then create a DataFrame for each.\n",
      "4. **Concatenate DataFrames**: Combine all individual DataFrames into one.\n",
      "5. **Handle potential errors**: Check if tables exist and handle cases where data might be missing or improperly structured.\n",
      "\n",
      "I need to make sure the code is robust. For example, checking if there are any tables before attempting to loop through them. Also, using try-except blocks around the main fetching and parsing logic to catch any exceptions that might occur during the process.\n",
      "\n",
      "When writing the DataFrames to a CSV, I should ensure that all data is correctly captured without any formatting issues. Using pandas' to_csv() function with appropriate parameters will handle this.\n",
      "\n",
      "I'll structure the code step by step, making sure each part is clear and commented where necessary for readability. Including pip install commands at the beginning ensures that the user has all the required libraries installed before running the script.\n",
      "\n",
      "Finally, I'll test the code mentally to ensure it covers all edge cases, like empty tables or network issues, but since it's a static URL, those might not be major concerns here.\n",
      "</think>\n",
      "\n",
      "I'll help you with web scraping and data processing. Let's break this down step by step:\n",
      "\n",
      "1. First, we need to scrape the webpage\n",
      "2. Then extract tables\n",
      "3. Convert them to DataFrames\n",
      "4. Concatenate and save to CSV\n",
      "\n",
      "Here's the Python code to accomplish this:\n",
      "\n",
      "```python\n",
      "# filename: webscraper.py\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "import pandas as pd\n",
      "import sys\n",
      "\n",
      "try:\n",
      "    # Get the webpage content\n",
      "    response = requests.get('https://neetcode.io/practice?tab=neetcode150')\n",
      "    if not response.status_code == 200:\n",
      "        print(f\"Failed to retrieve page. Status code: {response.status_code}\")\n",
      "        sys.exit(1)\n",
      "        \n",
      "    # Parse the HTML content\n",
      "    soup = BeautifulSoup(response.text, 'html.parser')\n",
      "    \n",
      "    # Find all table elements\n",
      "    tables = soup.find_all('table')\n",
      "    \n",
      "    if not tables:\n",
      "        print(\"No tables found on the page\")\n",
      "        sys.exit(0)\n",
      "    \n",
      "    dfs = []\n",
      "    for table in tables:\n",
      "        try:\n",
      "            # Extract headers and rows from the table\n",
      "            headers = [th.text.strip() for th in table.find('tr').find_all('th')]\n",
      "            rows = []\n",
      "            for tr in table.find_all('tr')[1:]:  # Skip header row\n",
      "                row = [td.text.strip() for td in tr.find_all('td')]\n",
      "                rows.append(row)\n",
      "                \n",
      "            # Create DataFrame and add to list\n",
      "            df = pd.DataFrame(rows, columns=headers)\n",
      "            dfs.append(df)\n",
      "        except Exception as e:\n",
      "            print(f\"Error processing table: {e}\")\n",
      "    \n",
      "    # Concatenate all DataFrames\n",
      "    if dfs:\n",
      "        combined_df = pd.concat(dfs)\n",
      "        # Save to CSV\n",
      "        combined_df.to_csv('neetcode_tables.csv', index=False, encoding='utf-8')\n",
      "        print(\"Successfully saved to neetcode_tables.csv\")\n",
      "    else:\n",
      "        print(\"No valid tables found to process\")\n",
      "\n",
      "except Exception as e:\n",
      "    print(f\"An error occurred: {e}\")\n",
      "```\n",
      "\n",
      "Before running this script, make sure you have the required packages installed:\n",
      "\n",
      "```bash\n",
      "pip install requests beautifulsoup4 pandas\n",
      "```\n",
      "\n",
      "This code will:\n",
      "1. Fetch the webpage content using requests\n",
      "2. Parse HTML to find all tables\n",
      "3. Convert each table into a DataFrame\n",
      "4. Combine all DataFrames into one\n",
      "5. Save the combined DataFrame to a CSV file named 'neetcode_tables.csv'\n",
      "\n",
      "The script includes error handling to manage potential issues during execution and will print appropriate messages about its progress.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> NO HUMAN INPUT RECEIVED.\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING 2 CODE BLOCKS (inferred languages are [python, bash])...\u001b[0m\n",
      "\u001b[33mcode_executor_agent\u001b[0m (to code_writer):\n",
      "\n",
      "exitcode: 1 (execution failed)\n",
      "Code output: No tables found on the page\n",
      "W\u0000i\u0000n\u0000d\u0000o\u0000w\u0000s\u0000 \u0000S\u0000u\u0000b\u0000s\u0000y\u0000s\u0000t\u0000e\u0000m\u0000 \u0000f\u0000o\u0000r\u0000 \u0000L\u0000i\u0000n\u0000u\u0000x\u0000 \u0000h\u0000a\u0000s\u0000 \u0000n\u0000o\u0000 \u0000i\u0000n\u0000s\u0000t\u0000a\u0000l\u0000l\u0000e\u0000d\u0000 \u0000d\u0000i\u0000s\u0000t\u0000r\u0000i\u0000b\u0000u\u0000t\u0000i\u0000o\u0000n\u0000s\u0000.\u0000\n",
      "\u0000\n",
      "\u0000\n",
      "\u0000\n",
      "\u0000U\u0000s\u0000e\u0000 \u0000'\u0000w\u0000s\u0000l\u0000.\u0000e\u0000x\u0000e\u0000 \u0000-\u0000-\u0000l\u0000i\u0000s\u0000t\u0000 \u0000-\u0000-\u0000o\u0000n\u0000l\u0000i\u0000n\u0000e\u0000'\u0000 \u0000t\u0000o\u0000 \u0000l\u0000i\u0000s\u0000t\u0000 \u0000a\u0000v\u0000a\u0000i\u0000l\u0000a\u0000b\u0000l\u0000e\u0000 \u0000d\u0000i\u0000s\u0000t\u0000r\u0000i\u0000b\u0000u\u0000t\u0000i\u0000o\u0000n\u0000s\u0000\n",
      "\u0000\n",
      "\u0000a\u0000n\u0000d\u0000 \u0000'\u0000w\u0000s\u0000l\u0000.\u0000e\u0000x\u0000e\u0000 \u0000-\u0000-\u0000i\u0000n\u0000s\u0000t\u0000a\u0000l\u0000l\u0000 \u0000<\u0000D\u0000i\u0000s\u0000t\u0000r\u0000o\u0000>\u0000'\u0000 \u0000t\u0000o\u0000 \u0000i\u0000n\u0000s\u0000t\u0000a\u0000l\u0000l\u0000.\u0000\n",
      "\u0000\n",
      "\u0000\n",
      "\u0000\n",
      "\u0000D\u0000i\u0000s\u0000t\u0000r\u0000i\u0000b\u0000u\u0000t\u0000i\u0000o\u0000n\u0000s\u0000 \u0000c\u0000a\u0000n\u0000 \u0000a\u0000l\u0000s\u0000o\u0000 \u0000b\u0000e\u0000 \u0000i\u0000n\u0000s\u0000t\u0000a\u0000l\u0000l\u0000e\u0000d\u0000 \u0000b\u0000y\u0000 \u0000v\u0000i\u0000s\u0000i\u0000t\u0000i\u0000n\u0000g\u0000 \u0000t\u0000h\u0000e\u0000 \u0000M\u0000i\u0000c\u0000r\u0000o\u0000s\u0000o\u0000f\u0000t\u0000 \u0000S\u0000t\u0000o\u0000r\u0000e\u0000:\u0000\n",
      "\u0000\n",
      "\u0000h\u0000t\u0000t\u0000p\u0000s\u0000:\u0000/\u0000/\u0000a\u0000k\u0000a\u0000.\u0000m\u0000s\u0000/\u0000w\u0000s\u0000l\u0000s\u0000t\u0000o\u0000r\u0000e\u0000\n",
      "\u0000\n",
      "\u0000E\u0000r\u0000r\u0000o\u0000r\u0000 \u0000c\u0000o\u0000d\u0000e\u0000:\u0000 \u0000B\u0000a\u0000s\u0000h\u0000/\u0000S\u0000e\u0000r\u0000v\u0000i\u0000c\u0000e\u0000/\u0000C\u0000r\u0000e\u0000a\u0000t\u0000e\u0000I\u0000n\u0000s\u0000t\u0000a\u0000n\u0000c\u0000e\u0000/\u0000G\u0000e\u0000t\u0000D\u0000e\u0000f\u0000a\u0000u\u0000l\u0000t\u0000D\u0000i\u0000s\u0000t\u0000r\u0000o\u0000/\u0000W\u0000S\u0000L\u0000_\u0000E\u0000_\u0000D\u0000E\u0000F\u0000A\u0000U\u0000L\u0000T\u0000_\u0000D\u0000I\u0000S\u0000T\u0000R\u0000O\u0000_\u0000N\u0000O\u0000T\u0000_\u0000F\u0000O\u0000U\u0000N\u0000D\u0000\n",
      "\u0000\n",
      "\u0000\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "ChatResult(chat_id=None,\n",
      "           chat_history=[{'content': '\\n'\n",
      "                                     '    Scrape the contents from this URL: '\n",
      "                                     'https://neetcode.io/practice?tab=neetcode150\\n'\n",
      "                                     '    Fetch all the <tables> tags '\n",
      "                                     'available in the page. Convert them to '\n",
      "                                     'pandas dataframes.\\n'\n",
      "                                     '    Concat all the dataframes into a '\n",
      "                                     'single dataframe and write it to a CSV '\n",
      "                                     'file.\\n'\n",
      "                                     '    \\n'\n",
      "                                     '    ',\n",
      "                          'name': 'code_executor_agent',\n",
      "                          'role': 'assistant'},\n",
      "                         {'content': '<think>\\n'\n",
      "                                     'Alright, I need to figure out how to '\n",
      "                                     'help the user scrape the contents from '\n",
      "                                     'the given URL using Python. The task is '\n",
      "                                     'to fetch all the <table> tags, convert '\n",
      "                                     'them into pandas DataFrames, concatenate '\n",
      "                                     'them, and then save the result into a '\n",
      "                                     'CSV file.\\n'\n",
      "                                     '\\n'\n",
      "                                     \"First, I'll consider the tools needed \"\n",
      "                                     'for web scraping. Beautiful Soup is a '\n",
      "                                     'good choice for parsing HTML content. To '\n",
      "                                     'fetch the webpage, requests library will '\n",
      "                                     'be used. For handling data, pandas is '\n",
      "                                     \"essential since we're dealing with \"\n",
      "                                     'tables and DataFrames.\\n'\n",
      "                                     '\\n'\n",
      "                                     'I should outline the steps clearly:\\n'\n",
      "                                     '\\n'\n",
      "                                     '1. **Fetch the webpage content**: Use '\n",
      "                                     'requests.get() to retrieve the HTML from '\n",
      "                                     'the URL.\\n'\n",
      "                                     '2. **Parse the HTML**: Use Beautiful '\n",
      "                                     'Soup to parse the HTML content and find '\n",
      "                                     'all table elements.\\n'\n",
      "                                     '3. **Convert tables to DataFrames**: '\n",
      "                                     'Loop through each table, extract rows '\n",
      "                                     'and columns, then create a DataFrame for '\n",
      "                                     'each.\\n'\n",
      "                                     '4. **Concatenate DataFrames**: Combine '\n",
      "                                     'all individual DataFrames into one.\\n'\n",
      "                                     '5. **Handle potential errors**: Check if '\n",
      "                                     'tables exist and handle cases where data '\n",
      "                                     'might be missing or improperly '\n",
      "                                     'structured.\\n'\n",
      "                                     '\\n'\n",
      "                                     'I need to make sure the code is robust. '\n",
      "                                     'For example, checking if there are any '\n",
      "                                     'tables before attempting to loop through '\n",
      "                                     'them. Also, using try-except blocks '\n",
      "                                     'around the main fetching and parsing '\n",
      "                                     'logic to catch any exceptions that might '\n",
      "                                     'occur during the process.\\n'\n",
      "                                     '\\n'\n",
      "                                     'When writing the DataFrames to a CSV, I '\n",
      "                                     'should ensure that all data is correctly '\n",
      "                                     'captured without any formatting issues. '\n",
      "                                     \"Using pandas' to_csv() function with \"\n",
      "                                     'appropriate parameters will handle '\n",
      "                                     'this.\\n'\n",
      "                                     '\\n'\n",
      "                                     \"I'll structure the code step by step, \"\n",
      "                                     'making sure each part is clear and '\n",
      "                                     'commented where necessary for '\n",
      "                                     'readability. Including pip install '\n",
      "                                     'commands at the beginning ensures that '\n",
      "                                     'the user has all the required libraries '\n",
      "                                     'installed before running the script.\\n'\n",
      "                                     '\\n'\n",
      "                                     \"Finally, I'll test the code mentally to \"\n",
      "                                     'ensure it covers all edge cases, like '\n",
      "                                     'empty tables or network issues, but '\n",
      "                                     \"since it's a static URL, those might not \"\n",
      "                                     'be major concerns here.\\n'\n",
      "                                     '</think>\\n'\n",
      "                                     '\\n'\n",
      "                                     \"I'll help you with web scraping and data \"\n",
      "                                     \"processing. Let's break this down step \"\n",
      "                                     'by step:\\n'\n",
      "                                     '\\n'\n",
      "                                     '1. First, we need to scrape the webpage\\n'\n",
      "                                     '2. Then extract tables\\n'\n",
      "                                     '3. Convert them to DataFrames\\n'\n",
      "                                     '4. Concatenate and save to CSV\\n'\n",
      "                                     '\\n'\n",
      "                                     \"Here's the Python code to accomplish \"\n",
      "                                     'this:\\n'\n",
      "                                     '\\n'\n",
      "                                     '```python\\n'\n",
      "                                     '# filename: webscraper.py\\n'\n",
      "                                     'import requests\\n'\n",
      "                                     'from bs4 import BeautifulSoup\\n'\n",
      "                                     'import pandas as pd\\n'\n",
      "                                     'import sys\\n'\n",
      "                                     '\\n'\n",
      "                                     'try:\\n'\n",
      "                                     '    # Get the webpage content\\n'\n",
      "                                     '    response = '\n",
      "                                     \"requests.get('https://neetcode.io/practice?tab=neetcode150')\\n\"\n",
      "                                     '    if not response.status_code == 200:\\n'\n",
      "                                     '        print(f\"Failed to retrieve page. '\n",
      "                                     'Status code: {response.status_code}\")\\n'\n",
      "                                     '        sys.exit(1)\\n'\n",
      "                                     '        \\n'\n",
      "                                     '    # Parse the HTML content\\n'\n",
      "                                     '    soup = BeautifulSoup(response.text, '\n",
      "                                     \"'html.parser')\\n\"\n",
      "                                     '    \\n'\n",
      "                                     '    # Find all table elements\\n'\n",
      "                                     \"    tables = soup.find_all('table')\\n\"\n",
      "                                     '    \\n'\n",
      "                                     '    if not tables:\\n'\n",
      "                                     '        print(\"No tables found on the '\n",
      "                                     'page\")\\n'\n",
      "                                     '        sys.exit(0)\\n'\n",
      "                                     '    \\n'\n",
      "                                     '    dfs = []\\n'\n",
      "                                     '    for table in tables:\\n'\n",
      "                                     '        try:\\n'\n",
      "                                     '            # Extract headers and rows '\n",
      "                                     'from the table\\n'\n",
      "                                     '            headers = [th.text.strip() '\n",
      "                                     'for th in '\n",
      "                                     \"table.find('tr').find_all('th')]\\n\"\n",
      "                                     '            rows = []\\n'\n",
      "                                     '            for tr in '\n",
      "                                     \"table.find_all('tr')[1:]:  # Skip header \"\n",
      "                                     'row\\n'\n",
      "                                     '                row = [td.text.strip() '\n",
      "                                     \"for td in tr.find_all('td')]\\n\"\n",
      "                                     '                rows.append(row)\\n'\n",
      "                                     '                \\n'\n",
      "                                     '            # Create DataFrame and add '\n",
      "                                     'to list\\n'\n",
      "                                     '            df = pd.DataFrame(rows, '\n",
      "                                     'columns=headers)\\n'\n",
      "                                     '            dfs.append(df)\\n'\n",
      "                                     '        except Exception as e:\\n'\n",
      "                                     '            print(f\"Error processing '\n",
      "                                     'table: {e}\")\\n'\n",
      "                                     '    \\n'\n",
      "                                     '    # Concatenate all DataFrames\\n'\n",
      "                                     '    if dfs:\\n'\n",
      "                                     '        combined_df = pd.concat(dfs)\\n'\n",
      "                                     '        # Save to CSV\\n'\n",
      "                                     '        '\n",
      "                                     \"combined_df.to_csv('neetcode_tables.csv', \"\n",
      "                                     \"index=False, encoding='utf-8')\\n\"\n",
      "                                     '        print(\"Successfully saved to '\n",
      "                                     'neetcode_tables.csv\")\\n'\n",
      "                                     '    else:\\n'\n",
      "                                     '        print(\"No valid tables found to '\n",
      "                                     'process\")\\n'\n",
      "                                     '\\n'\n",
      "                                     'except Exception as e:\\n'\n",
      "                                     '    print(f\"An error occurred: {e}\")\\n'\n",
      "                                     '```\\n'\n",
      "                                     '\\n'\n",
      "                                     'Before running this script, make sure '\n",
      "                                     'you have the required packages '\n",
      "                                     'installed:\\n'\n",
      "                                     '\\n'\n",
      "                                     '```bash\\n'\n",
      "                                     'pip install requests beautifulsoup4 '\n",
      "                                     'pandas\\n'\n",
      "                                     '```\\n'\n",
      "                                     '\\n'\n",
      "                                     'This code will:\\n'\n",
      "                                     '1. Fetch the webpage content using '\n",
      "                                     'requests\\n'\n",
      "                                     '2. Parse HTML to find all tables\\n'\n",
      "                                     '3. Convert each table into a DataFrame\\n'\n",
      "                                     '4. Combine all DataFrames into one\\n'\n",
      "                                     '5. Save the combined DataFrame to a CSV '\n",
      "                                     \"file named 'neetcode_tables.csv'\\n\"\n",
      "                                     '\\n'\n",
      "                                     'The script includes error handling to '\n",
      "                                     'manage potential issues during execution '\n",
      "                                     'and will print appropriate messages '\n",
      "                                     'about its progress.',\n",
      "                          'name': 'code_writer',\n",
      "                          'role': 'user'},\n",
      "                         {'content': 'exitcode: 1 (execution failed)\\n'\n",
      "                                     'Code output: No tables found on the '\n",
      "                                     'page\\n'\n",
      "                                     'W\\x00i\\x00n\\x00d\\x00o\\x00w\\x00s\\x00 '\n",
      "                                     '\\x00S\\x00u\\x00b\\x00s\\x00y\\x00s\\x00t\\x00e\\x00m\\x00 '\n",
      "                                     '\\x00f\\x00o\\x00r\\x00 '\n",
      "                                     '\\x00L\\x00i\\x00n\\x00u\\x00x\\x00 '\n",
      "                                     '\\x00h\\x00a\\x00s\\x00 \\x00n\\x00o\\x00 '\n",
      "                                     '\\x00i\\x00n\\x00s\\x00t\\x00a\\x00l\\x00l\\x00e\\x00d\\x00 '\n",
      "                                     '\\x00d\\x00i\\x00s\\x00t\\x00r\\x00i\\x00b\\x00u\\x00t\\x00i\\x00o\\x00n\\x00s\\x00.\\x00\\n'\n",
      "                                     '\\x00\\n'\n",
      "                                     '\\x00\\n'\n",
      "                                     '\\x00\\n'\n",
      "                                     '\\x00U\\x00s\\x00e\\x00 '\n",
      "                                     \"\\x00'\\x00w\\x00s\\x00l\\x00.\\x00e\\x00x\\x00e\\x00 \"\n",
      "                                     '\\x00-\\x00-\\x00l\\x00i\\x00s\\x00t\\x00 '\n",
      "                                     \"\\x00-\\x00-\\x00o\\x00n\\x00l\\x00i\\x00n\\x00e\\x00'\\x00 \"\n",
      "                                     '\\x00t\\x00o\\x00 \\x00l\\x00i\\x00s\\x00t\\x00 '\n",
      "                                     '\\x00a\\x00v\\x00a\\x00i\\x00l\\x00a\\x00b\\x00l\\x00e\\x00 '\n",
      "                                     '\\x00d\\x00i\\x00s\\x00t\\x00r\\x00i\\x00b\\x00u\\x00t\\x00i\\x00o\\x00n\\x00s\\x00\\n'\n",
      "                                     '\\x00\\n'\n",
      "                                     '\\x00a\\x00n\\x00d\\x00 '\n",
      "                                     \"\\x00'\\x00w\\x00s\\x00l\\x00.\\x00e\\x00x\\x00e\\x00 \"\n",
      "                                     '\\x00-\\x00-\\x00i\\x00n\\x00s\\x00t\\x00a\\x00l\\x00l\\x00 '\n",
      "                                     \"\\x00<\\x00D\\x00i\\x00s\\x00t\\x00r\\x00o\\x00>\\x00'\\x00 \"\n",
      "                                     '\\x00t\\x00o\\x00 '\n",
      "                                     '\\x00i\\x00n\\x00s\\x00t\\x00a\\x00l\\x00l\\x00.\\x00\\n'\n",
      "                                     '\\x00\\n'\n",
      "                                     '\\x00\\n'\n",
      "                                     '\\x00\\n'\n",
      "                                     '\\x00D\\x00i\\x00s\\x00t\\x00r\\x00i\\x00b\\x00u\\x00t\\x00i\\x00o\\x00n\\x00s\\x00 '\n",
      "                                     '\\x00c\\x00a\\x00n\\x00 '\n",
      "                                     '\\x00a\\x00l\\x00s\\x00o\\x00 \\x00b\\x00e\\x00 '\n",
      "                                     '\\x00i\\x00n\\x00s\\x00t\\x00a\\x00l\\x00l\\x00e\\x00d\\x00 '\n",
      "                                     '\\x00b\\x00y\\x00 '\n",
      "                                     '\\x00v\\x00i\\x00s\\x00i\\x00t\\x00i\\x00n\\x00g\\x00 '\n",
      "                                     '\\x00t\\x00h\\x00e\\x00 '\n",
      "                                     '\\x00M\\x00i\\x00c\\x00r\\x00o\\x00s\\x00o\\x00f\\x00t\\x00 '\n",
      "                                     '\\x00S\\x00t\\x00o\\x00r\\x00e\\x00:\\x00\\n'\n",
      "                                     '\\x00\\n'\n",
      "                                     '\\x00h\\x00t\\x00t\\x00p\\x00s\\x00:\\x00/\\x00/\\x00a\\x00k\\x00a\\x00.\\x00m\\x00s\\x00/\\x00w\\x00s\\x00l\\x00s\\x00t\\x00o\\x00r\\x00e\\x00\\n'\n",
      "                                     '\\x00\\n'\n",
      "                                     '\\x00E\\x00r\\x00r\\x00o\\x00r\\x00 '\n",
      "                                     '\\x00c\\x00o\\x00d\\x00e\\x00:\\x00 '\n",
      "                                     '\\x00B\\x00a\\x00s\\x00h\\x00/\\x00S\\x00e\\x00r\\x00v\\x00i\\x00c\\x00e\\x00/\\x00C\\x00r\\x00e\\x00a\\x00t\\x00e\\x00I\\x00n\\x00s\\x00t\\x00a\\x00n\\x00c\\x00e\\x00/\\x00G\\x00e\\x00t\\x00D\\x00e\\x00f\\x00a\\x00u\\x00l\\x00t\\x00D\\x00i\\x00s\\x00t\\x00r\\x00o\\x00/\\x00W\\x00S\\x00L\\x00_\\x00E\\x00_\\x00D\\x00E\\x00F\\x00A\\x00U\\x00L\\x00T\\x00_\\x00D\\x00I\\x00S\\x00T\\x00R\\x00O\\x00_\\x00N\\x00O\\x00T\\x00_\\x00F\\x00O\\x00U\\x00N\\x00D\\x00\\n'\n",
      "                                     '\\x00\\n'\n",
      "                                     '\\x00',\n",
      "                          'name': 'code_executor_agent',\n",
      "                          'role': 'assistant'}],\n",
      "           summary='exitcode: 1 (execution failed)\\n'\n",
      "                   'Code output: No tables found on the page\\n'\n",
      "                   'W\\x00i\\x00n\\x00d\\x00o\\x00w\\x00s\\x00 '\n",
      "                   '\\x00S\\x00u\\x00b\\x00s\\x00y\\x00s\\x00t\\x00e\\x00m\\x00 '\n",
      "                   '\\x00f\\x00o\\x00r\\x00 \\x00L\\x00i\\x00n\\x00u\\x00x\\x00 '\n",
      "                   '\\x00h\\x00a\\x00s\\x00 \\x00n\\x00o\\x00 '\n",
      "                   '\\x00i\\x00n\\x00s\\x00t\\x00a\\x00l\\x00l\\x00e\\x00d\\x00 '\n",
      "                   '\\x00d\\x00i\\x00s\\x00t\\x00r\\x00i\\x00b\\x00u\\x00t\\x00i\\x00o\\x00n\\x00s\\x00.\\x00\\n'\n",
      "                   '\\x00\\n'\n",
      "                   '\\x00\\n'\n",
      "                   '\\x00\\n'\n",
      "                   '\\x00U\\x00s\\x00e\\x00 '\n",
      "                   \"\\x00'\\x00w\\x00s\\x00l\\x00.\\x00e\\x00x\\x00e\\x00 \"\n",
      "                   '\\x00-\\x00-\\x00l\\x00i\\x00s\\x00t\\x00 '\n",
      "                   \"\\x00-\\x00-\\x00o\\x00n\\x00l\\x00i\\x00n\\x00e\\x00'\\x00 \"\n",
      "                   '\\x00t\\x00o\\x00 \\x00l\\x00i\\x00s\\x00t\\x00 '\n",
      "                   '\\x00a\\x00v\\x00a\\x00i\\x00l\\x00a\\x00b\\x00l\\x00e\\x00 '\n",
      "                   '\\x00d\\x00i\\x00s\\x00t\\x00r\\x00i\\x00b\\x00u\\x00t\\x00i\\x00o\\x00n\\x00s\\x00\\n'\n",
      "                   '\\x00\\n'\n",
      "                   '\\x00a\\x00n\\x00d\\x00 '\n",
      "                   \"\\x00'\\x00w\\x00s\\x00l\\x00.\\x00e\\x00x\\x00e\\x00 \"\n",
      "                   '\\x00-\\x00-\\x00i\\x00n\\x00s\\x00t\\x00a\\x00l\\x00l\\x00 '\n",
      "                   \"\\x00<\\x00D\\x00i\\x00s\\x00t\\x00r\\x00o\\x00>\\x00'\\x00 \"\n",
      "                   '\\x00t\\x00o\\x00 '\n",
      "                   '\\x00i\\x00n\\x00s\\x00t\\x00a\\x00l\\x00l\\x00.\\x00\\n'\n",
      "                   '\\x00\\n'\n",
      "                   '\\x00\\n'\n",
      "                   '\\x00\\n'\n",
      "                   '\\x00D\\x00i\\x00s\\x00t\\x00r\\x00i\\x00b\\x00u\\x00t\\x00i\\x00o\\x00n\\x00s\\x00 '\n",
      "                   '\\x00c\\x00a\\x00n\\x00 \\x00a\\x00l\\x00s\\x00o\\x00 '\n",
      "                   '\\x00b\\x00e\\x00 '\n",
      "                   '\\x00i\\x00n\\x00s\\x00t\\x00a\\x00l\\x00l\\x00e\\x00d\\x00 '\n",
      "                   '\\x00b\\x00y\\x00 '\n",
      "                   '\\x00v\\x00i\\x00s\\x00i\\x00t\\x00i\\x00n\\x00g\\x00 '\n",
      "                   '\\x00t\\x00h\\x00e\\x00 '\n",
      "                   '\\x00M\\x00i\\x00c\\x00r\\x00o\\x00s\\x00o\\x00f\\x00t\\x00 '\n",
      "                   '\\x00S\\x00t\\x00o\\x00r\\x00e\\x00:\\x00\\n'\n",
      "                   '\\x00\\n'\n",
      "                   '\\x00h\\x00t\\x00t\\x00p\\x00s\\x00:\\x00/\\x00/\\x00a\\x00k\\x00a\\x00.\\x00m\\x00s\\x00/\\x00w\\x00s\\x00l\\x00s\\x00t\\x00o\\x00r\\x00e\\x00\\n'\n",
      "                   '\\x00\\n'\n",
      "                   '\\x00E\\x00r\\x00r\\x00o\\x00r\\x00 '\n",
      "                   '\\x00c\\x00o\\x00d\\x00e\\x00:\\x00 '\n",
      "                   '\\x00B\\x00a\\x00s\\x00h\\x00/\\x00S\\x00e\\x00r\\x00v\\x00i\\x00c\\x00e\\x00/\\x00C\\x00r\\x00e\\x00a\\x00t\\x00e\\x00I\\x00n\\x00s\\x00t\\x00a\\x00n\\x00c\\x00e\\x00/\\x00G\\x00e\\x00t\\x00D\\x00e\\x00f\\x00a\\x00u\\x00l\\x00t\\x00D\\x00i\\x00s\\x00t\\x00r\\x00o\\x00/\\x00W\\x00S\\x00L\\x00_\\x00E\\x00_\\x00D\\x00E\\x00F\\x00A\\x00U\\x00L\\x00T\\x00_\\x00D\\x00I\\x00S\\x00T\\x00R\\x00O\\x00_\\x00N\\x00O\\x00T\\x00_\\x00F\\x00O\\x00U\\x00N\\x00D\\x00\\n'\n",
      "                   '\\x00\\n'\n",
      "                   '\\x00',\n",
      "           cost={'usage_excluding_cached_inference': {'deepseek-r1:14b': {'completion_tokens': 934,\n",
      "                                                                          'cost': 0,\n",
      "                                                                          'prompt_tokens': 452,\n",
      "                                                                          'total_tokens': 1386},\n",
      "                                                      'total_cost': 0},\n",
      "                 'usage_including_cached_inference': {'deepseek-r1:14b': {'completion_tokens': 934,\n",
      "                                                                          'cost': 0,\n",
      "                                                                          'prompt_tokens': 452,\n",
      "                                                                          'total_tokens': 1386},\n",
      "                                                      'total_cost': 0}},\n",
      "           human_input=[''])\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "chat_result = code_executor_agent.initiate_chat(\n",
    "    code_writer_agent, message=\"\"\"\n",
    "    Scrape the contents from this URL: https://neetcode.io/practice?tab=neetcode150\n",
    "    Fetch all the <tables> tags available in the page. Convert them to pandas dataframes.\n",
    "    Concat all the dataframes into a single dataframe and write it to a CSV file.\n",
    "    \n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "pprint.pprint(chat_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
